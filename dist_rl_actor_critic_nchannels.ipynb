{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!conda create -n sarsa python=3.9 -y\n",
        "!conda activate sarsa\n",
        "!pip install \"gymnasium[atari]\" numpy matplotlib\n",
        "!pip install autorom[accept-rom-license]  # Downloading Gym env data files\n",
        "!AutoROM --accept-license  # Accepting the license for data files\n",
        "!pip install ipykernel  # Install Jupyter kernel manager\n",
        "!ipython kernel install --user --name=sarsa  # Add the new Conda env to Jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFO5n1HYUXbh",
        "outputId": "82999b0a-700a-4ea0-95b5-9aa1d35d62b7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:56:41.789326Z",
          "iopub.execute_input": "2025-01-31T21:56:41.789605Z",
          "iopub.status.idle": "2025-01-31T21:57:03.343717Z",
          "shell.execute_reply.started": "2025-01-31T21:56:41.789584Z",
          "shell.execute_reply": "2025-01-31T21:57:03.342691Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2024.12.14)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=bd4fa45d38aeca5fab53d64e2a99aa9103c10c14224e0a9a83367ad3665db6fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n",
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.0.0->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel) (1.17.0)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Installed kernelspec sarsa in /root/.local/share/jupyter/kernels/sarsa\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "5tZVriaWUYYp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:03.344882Z",
          "iopub.execute_input": "2025-01-31T21:57:03.345120Z",
          "iopub.status.idle": "2025-01-31T21:57:07.322623Z",
          "shell.execute_reply.started": "2025-01-31T21:57:03.345088Z",
          "shell.execute_reply": "2025-01-31T21:57:07.321967Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU\n",
        "import wandb\n",
        "import random\n",
        "import math\n"
      ],
      "metadata": {
        "id": "vpNTmcMkQZmC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:07.323971Z",
          "iopub.execute_input": "2025-01-31T21:57:07.324402Z",
          "iopub.status.idle": "2025-01-31T21:57:25.887095Z",
          "shell.execute_reply.started": "2025-01-31T21:57:07.324377Z",
          "shell.execute_reply": "2025-01-31T21:57:25.886342Z"
        },
        "outputId": "152e8617-180d-4a1e-d019-e61302827536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"6d31e0d6a0ebbac0b62eac799f098e4d1094cf52\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxvC6i-0SWIS",
        "outputId": "9cdea11f-e1e1-4174-9cde-6c824b035aac",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:25.888364Z",
          "iopub.execute_input": "2025-01-31T21:57:25.888598Z",
          "iopub.status.idle": "2025-01-31T21:57:31.692454Z",
          "shell.execute_reply.started": "2025-01-31T21:57:25.888579Z",
          "shell.execute_reply": "2025-01-31T21:57:31.691797Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpsingadi\u001b[0m (\u001b[33mGroupXV\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting up the environment"
      ],
      "metadata": {
        "id": "HILZ2Zpbf6N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "print(env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSClnkucVK61",
        "outputId": "ab14af86-3af4-45ea-8678-cfc707eae4b3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:31.693093Z",
          "iopub.execute_input": "2025-01-31T21:57:31.693485Z",
          "iopub.status.idle": "2025-01-31T21:57:31.718994Z",
          "shell.execute_reply.started": "2025-01-31T21:57:31.693461Z",
          "shell.execute_reply": "2025-01-31T21:57:31.718301Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the environment\n",
        "state = env.reset()\n",
        "for _ in range(100):\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        break\n",
        "env.close()"
      ],
      "metadata": {
        "id": "-H-Iqzw1Ubgp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:31.719720Z",
          "iopub.execute_input": "2025-01-31T21:57:31.719912Z",
          "iopub.status.idle": "2025-01-31T21:57:33.801899Z",
          "shell.execute_reply.started": "2025-01-31T21:57:31.719894Z",
          "shell.execute_reply": "2025-01-31T21:57:33.801207Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Actor policy\n",
        "it should outputs probabilities of each actions given the state using softmax"
      ],
      "metadata": {
        "id": "wZkiEDsyfzbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistActorCritic(nn.Module):\n",
        "    def __init__(self, env_input_dim, env_output_dim, n_hidden, theta_value,n_channels=3, lr_range=[0.0001,0.3]):\n",
        "        super(DistActorCritic, self).__init__()\n",
        "\n",
        "        self.n_channels = n_channels  # dimension of distributional value output, eg. 3 (pess, opt, real) or 8\n",
        "        self.n_hidden = n_hidden  # dimension of shared representation, eg. 64\n",
        "        self.lr_range = lr_range\n",
        "        self.env_input_dim = env_input_dim\n",
        "        self.env_output_dim = env_output_dim\n",
        "        self.theta_value = theta_value\n",
        "\n",
        "\n",
        "        self.alpha_plus = [3*theta_value, 2*theta_value, theta_value]\n",
        "        self.alpha_minus = [theta_value, 2*theta_value, 3*theta_value]\n",
        "\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(self.env_input_dim, self.n_hidden),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(self.n_hidden,self.env_output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(self.n_hidden, self.n_channels),\n",
        "            #nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nn(x)\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "VXLWQLc3zG9e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:33.837387Z",
          "iopub.execute_input": "2025-01-31T21:57:33.837705Z",
          "iopub.status.idle": "2025-01-31T21:57:33.853006Z",
          "shell.execute_reply.started": "2025-01-31T21:57:33.837674Z",
          "shell.execute_reply": "2025-01-31T21:57:33.852282Z"
        }
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "state, _ = env.reset()\n",
        "state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "dist_actor_critic = DistActorCritic(theta_value=0.1, env_input_dim=env.observation_space.shape[0], env_output_dim=env.action_space.n,\n",
        "                                    n_hidden=64, n_channels=8)\n",
        "\n",
        "print(\"state tensor is\",state_tensor.flatten(), state_tensor.shape)\n",
        "\n",
        "action_probs, values = dist_actor_critic(state_tensor)\n",
        "\n",
        "\n",
        "print(action_probs)\n",
        "print(values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcPFiHrHdTZ",
        "outputId": "5776386d-d524-49ea-e428-3fb51acb30fc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:33.894288Z",
          "iopub.execute_input": "2025-01-31T21:57:33.894589Z",
          "iopub.status.idle": "2025-01-31T21:57:34.633169Z",
          "shell.execute_reply.started": "2025-01-31T21:57:33.894558Z",
          "shell.execute_reply": "2025-01-31T21:57:34.632292Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state tensor is tensor([-0.0353,  0.0203,  0.0299, -0.0126]) torch.Size([1, 4])\n",
            "tensor([[0.5365, 0.4635]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[ 0.1166,  0.0410, -0.0161,  0.1398,  0.0789, -0.2104, -0.1928, -0.2280]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "# f(delta) functions\n",
        "def f(delta, k=1):\n",
        "    return torch.clamp(delta, min=-k, max=k)"
      ],
      "metadata": {
        "id": "Ceq3JF2zr1Hj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:34.634100Z",
          "iopub.execute_input": "2025-01-31T21:57:34.634435Z",
          "iopub.status.idle": "2025-01-31T21:57:34.638357Z",
          "shell.execute_reply.started": "2025-01-31T21:57:34.634401Z",
          "shell.execute_reply": "2025-01-31T21:57:34.637540Z"
        }
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Update critic------ values\n",
        "\n",
        "def update_critic(dist_actor_critic, state,next_state, reward, gamma, n_channels):\n",
        "    _, values = dist_actor_critic(state)\n",
        "    values = values\n",
        "    _, next_values = dist_actor_critic(next_state)\n",
        "    next_values = next_values\n",
        "\n",
        "    # finding ith values in range n_channels to calculate delta and update values for delta +ve or -ve\n",
        "    for i in range(n_channels):\n",
        "        # now sampling one of the value predictions on j random index withing n_channels range\n",
        "\n",
        "        delta = reward + gamma * next_values[0][i] - values[0][i]\n",
        "        if delta > 0:\n",
        "            values[0][i] += dist_actor_critic.alpha_plus[i] * f(delta)\n",
        "        else:\n",
        "            values[0][i] += dist_actor_critic.alpha_minus[i] * f(delta)\n",
        "\n",
        "    return values, delta\n"
      ],
      "metadata": {
        "id": "mjL-wQnrR5HV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:34.639187Z",
          "iopub.execute_input": "2025-01-31T21:57:34.639510Z",
          "iopub.status.idle": "2025-01-31T21:57:34.654097Z",
          "shell.execute_reply.started": "2025-01-31T21:57:34.639479Z",
          "shell.execute_reply": "2025-01-31T21:57:34.653367Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantage function and Actor-Critic algorithm\n"
      ],
      "metadata": {
        "id": "nChZvOO8iuLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_actor_critic(env, theta_value, n_hidden=64, num_episodes=1000, gamma=0.99, alpha=0.001, beta=0.001, max_timesteps=200):\n",
        "    # Initialize Critic and Actor networks\n",
        "\n",
        "    dist_actor_critic = DistActorCritic(theta_value=theta_value, env_input_dim=env.observation_space.shape[0], env_output_dim=env.action_space.n, n_hidden=n_hidden)\n",
        "\n",
        "    critic = dist_actor_critic.critic\n",
        "    actor = dist_actor_critic.actor\n",
        "\n",
        "\n",
        "    # Initialize optimizers\n",
        "\n",
        "    optimizer = optim.Adam(dist_actor_critic.nn.parameters(), lr=alpha)\n",
        "\n",
        "    # To store episode rewards\n",
        "    episode_rewards = []\n",
        "    episode_steps = []\n",
        "\n",
        "    ave_episodes = []\n",
        "    ave_steps = []\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "\n",
        "        env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "        state, _ = env.reset()\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "\n",
        "            action_probs, values = dist_actor_critic(state_tensor)\n",
        "            # Get action probabilities from the actor network\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "\n",
        "            # print(\"action is\",action)\n",
        "\n",
        "            # Take a step in the environment\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "\n",
        "            # Update critic and get advantage\n",
        "            values, delta = update_critic(dist_actor_critic, state_tensor, next_state_tensor, reward, gamma,\n",
        "                                         dist_actor_critic.n_channels)\n",
        "            advantage = torch.mean(delta)\n",
        "\n",
        "            # Calculate actor loss (policy gradient)\n",
        "            actor_loss = -torch.log(action_probs[0][action]) * advantage.detach()\n",
        "\n",
        "            # Calculate critic loss (mean squared error)\n",
        "            critic_loss = advantage ** 2\n",
        "\n",
        "\n",
        "            # calculate main network loss (ave of the two)\n",
        "            main_loss = actor_loss + critic_loss\n",
        "\n",
        "\n",
        "            # Update actor and critic\n",
        "            optimizer.zero_grad()\n",
        "            main_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update state and action probabilities\n",
        "            state_tensor = next_state_tensor\n",
        "            action_probs, values = dist_actor_critic(state_tensor)\n",
        "\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Store episode reward\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_steps.append(steps)\n",
        "\n",
        "        # Print progress every 10 episodes\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
        "            ave_episodes.append(episode)\n",
        "            ave_steps.append(steps)\n",
        "\n",
        "    return episode_rewards, episode_steps\n"
      ],
      "metadata": {
        "id": "dxXWIeRxsHEi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:34.654802Z",
          "iopub.execute_input": "2025-01-31T21:57:34.655007Z",
          "iopub.status.idle": "2025-01-31T21:57:34.665356Z",
          "shell.execute_reply.started": "2025-01-31T21:57:34.654988Z",
          "shell.execute_reply": "2025-01-31T21:57:34.664572Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def moving_average(data, window_size=10):\n",
        "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "def plot_learning_curve(episode_rewards, episode_lengths, theta_value, window_size=10):\n",
        "    # Compute smoothed values\n",
        "    smoothed_rewards = moving_average(episode_rewards, window_size)\n",
        "    smoothed_lengths = moving_average(episode_lengths, window_size)\n",
        "\n",
        "    # Logging to WandB\n",
        "    with wandb.init(project=\"ProjectName\", name=f\"theta_{theta_value}\"):\n",
        "        for ep in range(len(smoothed_rewards)):\n",
        "            wandb.log({\n",
        "                \"theta\": theta_value,\n",
        "                \"episode\": ep,\n",
        "                \"reward\": smoothed_rewards[ep],\n",
        "                \"steps\": smoothed_lengths[ep]\n",
        "            })\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(smoothed_rewards, label=\"Smoothed Rewards\", color='b', linewidth=2)\n",
        "    plt.plot(smoothed_lengths, label=\"Smoothed Episode Lengths\", color='r', linestyle=\"dashed\", linewidth=2)\n",
        "\n",
        "    plt.title(f\"Smoothed Learning Curve (Theta={theta_value})\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_value_function_convergence(value_estimates, theta_value):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(moving_average(value_estimates, 10), color='purple', linewidth=2)\n",
        "\n",
        "    plt.title(f\"Value Function Convergence (Theta={theta_value})\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Estimated Value\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_reward_distribution(episode_rewards, theta_value):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(episode_rewards, bins=20, color='blue', alpha=0.7, edgecolor='black')\n",
        "\n",
        "    plt.title(f\"Reward Distribution (Theta={theta_value})\")\n",
        "    plt.xlabel(\"Total Reward per Episode\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_reward_variance(episode_rewards, theta_value, window_size=10):\n",
        "    std_rewards = [np.std(episode_rewards[max(0, i-window_size):i+1]) for i in range(len(episode_rewards))]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(std_rewards, color='red', linewidth=2)\n",
        "\n",
        "    plt.title(f\"Reward Variance Over Time (Theta={theta_value})\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Standard Deviation of Rewards\")\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "J1billXl5r0o",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:34.666185Z",
          "iopub.execute_input": "2025-01-31T21:57:34.666479Z",
          "iopub.status.idle": "2025-01-31T21:57:34.686047Z",
          "shell.execute_reply.started": "2025-01-31T21:57:34.666452Z",
          "shell.execute_reply": "2025-01-31T21:57:34.685069Z"
        }
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "theta = [0.03,3]\n",
        "for i in range(len(theta)):\n",
        "    theta_value = theta[i]\n",
        "    #train_actor_critic(env, theta_value, num_episodes= 200)\n",
        "    episode_rewards, episode_steps = train_actor_critic(env, theta_value, num_episodes= 800)\n",
        "    plot_learning_curve(episode_rewards, episode_steps, theta_value)\n",
        "    plot_reward_distribution(episode_rewards, theta_value)\n",
        "    plot_reward_variance(episode_rewards, theta_value)\n",
        "    plot_value_function_convergence(episode_rewards, theta_value)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY9fwVboPryi",
        "outputId": "5122c49d-9d4e-4299-b797-e23e192b1fbb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T21:57:34.686782Z",
          "iopub.execute_input": "2025-01-31T21:57:34.687047Z",
          "execution_failed": "2025-01-31T22:13:10.765Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: 28.0\n",
            "Episode 10, Reward: 11.0\n",
            "Episode 20, Reward: 28.0\n",
            "Episode 30, Reward: 17.0\n",
            "Episode 40, Reward: 11.0\n",
            "Episode 50, Reward: 11.0\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "don't want to touch the results below"
      ],
      "metadata": {
        "id": "9QkK0jSKlXCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the learning curve\n",
        "def plot_learning_curve(episode_rewards, episode_lengths, theta_value):\n",
        "\n",
        "\n",
        "    with wandb.init(project=\"CartPole-v1\", name=f\"theta_{theta_value}\"):\n",
        "\n",
        "        for ep in range(len(episode_rewards)):\n",
        "            wandb.log({\n",
        "                \"theta\": theta_value,\n",
        "                \"episode\": ep,\n",
        "                \"reward\": episode_rewards[ep],\n",
        "                \"steps\": episode_steps[ep]\n",
        "            })\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.title(\"Episode Rewards\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(episode_lengths)\n",
        "    plt.title(\"Episode Lengths\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Number of Steps\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3sHb1HDrvT0e",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-01-31T22:13:10.767Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# RL seems to be sensitive to lr weights, we chose to experiment with different weights of theta\n",
        "theta = [0.001, 0.005, 0.01, 0.05]\n",
        "for i in range(len(theta)):\n",
        "    theta_value = theta[i]\n",
        "    #train_actor_critic(env, theta_value, num_episodes= 200)\n",
        "    episode_rewards, episode_steps = train_actor_critic(env, theta_value, num_episodes= 300)\n",
        "    plot_learning_curve(episode_rewards, episode_steps, theta_value)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4b4zg1yVFEaf",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-01-31T22:13:10.768Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n"
      ],
      "metadata": {
        "id": "fvfcIUtNmMnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anallysis of results\n",
        "The low rewards at the beginning of the training:\n",
        "\n",
        "*  the agent is learning as it explores the environment more since it's actor's policy is essentially random *it does't know yet which actions gives high reward*\n",
        "\n",
        "* the other posible cause could be the untrained critic network during early stages of learning resulting in making high inaccurate predictions for state values [the advantage function will be noisy]\n",
        "\n",
        "* also random initialization in actions by the Actor network don't really give high rewards in CartPole game as the goal is to find a balance a pole on moving cart\n",
        "\n",
        "\n",
        "The drop of rewards after stabilization:\n",
        "* over shooting the optimal point during graduate descent or ascent when alpha and beta are set too high\n",
        "* overfitting after the policy stabilizes during early stage\n",
        "* policy gradient noise when advantange function is too high\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ8Yutgf5kpB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhP8Mj2w0w-w",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}